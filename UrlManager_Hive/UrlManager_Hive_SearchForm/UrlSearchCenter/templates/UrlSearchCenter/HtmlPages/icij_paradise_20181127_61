<div><div><h2>-</h2></div><div>the thing about investigative reporting is it\xe2\x80\x99s hard work. sure we\xe2\x80\x99ve got more data available now. but data presents its own challenges: you\xe2\x80\x99re tackling a massive pile of information looking for the few best bits. a technique called web scraping can help you extract information from a website that otherwise is not easily downloadable using a piece of code or a program. web scraping gives you access to information living on the internet. if you can view it on a website you can harvest it. and since you can collect it you might as well automate that process for large datasets \xe2\x80\x94 at least if the website\xe2\x80\x99s terms and conditions don\xe2\x80\x99t say otherwise. and it really helps. \xe2\x80\x9cyou might go to an agency&#8217;s website to get some data you\xe2\x80\x99re interested in but the way they&#8217;ve got their web app set up you&#8217;ve got to click through 3000 pages to get all of the information\xe2\x80\x9d said <a href="https://www.ire.org/">investigative reporters and editors</a> training director cody winchester what\xe2\x80\x99s the solution  web scraping. you can write a script in a coding language (<a href="https://www.python.org/">python</a> is one) that funnels the desired information into a spreadsheet and automatically flicks through all of the pages. or you could bypass coding completely and use an application to deal swiftly with the web scraping for example <a href="https://www.outwit.com/products/hub/">outwit hub</a> a point and click tool that recognizes online elements and downloads and organizes them into datasets. web scraping gives reporters the ability to create their own datasets with scraped information opening the possibility of discovering new stories \xe2\x80\x94 a priority for investigative journalists. jodi upton the knight chair of data and explanatory journalism at syracuse university began her career doing old-school \xe2\x80\x9cscraping.\xe2\x80\x9d before online databases were widely used when she only had access to paper records she created her own databases manually. for upton\xe2\x80\x99s work it was a necessity. \xe2\x80\x9cwhen you\xe2\x80\x99re trying to do news stories or investigative projects that require really interesting data often it means you are creating a database yourself\xe2\x80\x9d upton said. now it\xe2\x80\x99s a lot easier though the raw product data itself isn\xe2\x80\x99t always easy to get your hands on. there isn\xe2\x80\x99t much incentive for organizations to disclose important data unless required to by law. even then the government does a poor job of data maintenance. \xe2\x80\x9cwe do have some data from the government but we know that it is so inaccurately kept that there are some really good stories in finding out just how wrong they are\xe2\x80\x9d upton said. working on usa today\xe2\x80\x99s <a href="http://www.gannett-cdn.com/gdcontent/mass-killings/index.html#title">mass killings</a> project an investigation into federal bureau of investigation mass homicide data upton and the rest of the data team scoured fbi data for mass homicides. the data was so poorly kept that the team had to hand-check and verify every incident itself. they found many more incidents the fbi had failed to log. upton said she was concerned. \xe2\x80\x9cthis is our premiere crime fighting agency in the u.s. and when it comes to mass killings they&#8217;re right around only 57 percent of the time.\xe2\x80\x9d sometimes the government will simply refuse to hand over data sets. ire&#8217;s winchester described his attempt to get a database from a south dakota government lobbyist who argued that putting data up on a webpage was transparent enough: \xe2\x80\x9ci put in a records request to get the data in the database that was powering their web app and they successfully argued &#8216;we&#8217;re already making the information available we don&#8217;t have to do anything special to give it to you as data\xe2\x80\x99.\xe2\x80\x9d aside from structured data which is organized to make it more accessible some stories are born from journalists giving structure to unstructured information. in 2013 reuters investigated a <a href="https://www.reuters.com/investigates/adoption/#article/part1">marketplace for adopted children</a> who were being offered by the parents or guardians who had taken them in on yahoo message boards to strangers. the investigative team scraped the message boards and found 261 children on offer. the team was then able to organize the children by gender age nationality and by their \xe2\x80\x94situations such as having special needs or a history of abuse. \xe2\x80\x9cthat is not a dataset that a government agency produces. that is not a dataset that is easy to obtain in any way. it was just scraping effectively; a social media scraping\xe2\x80\x9d upton said. samantha sunne a freelance data and investigative reporter created a whole tutorial for those without coding experience. \xe2\x80\x9cwhen i&#8217;m investigating stories as a reporter i don&#8217;t actually write code that often\xe2\x80\x9d sunne said. instead she uses google sheets to scrape tables and lists off a single page using a simple formula within the program. the formula imports a few html elements into google sheets and is easy enough for anyone with basic html knowledge to follow. you can read her entire tutorial <a href="https://docs.google.com/presentation/d/1zmvbeyhzzmv_pbsxijavwjr9i2uujegp76djqoa_-u4/pub start=false& loop=false& delayms=60000& slide=id.g2f494677ec_0_11">here</a>. \xe2\x80\x9ci&#8217;ve used it for court documents at a local courthouse i use it for job postings for a newsletter i write about journalism happenings\xe2\x80\x9d sunne said. \xe2\x80\x9cit&#8217;s a spreadsheet that automatically updates from like 30 different job boards. it makes the most sense for things that continually update like that.\xe2\x80\x9d <iframe src="https://docs.google.com/presentation/d/e/2pacx-1vrzjgespudqrypdzo3ypq0aodnzl4gkag27ctyzfrfksd_gazpjwnwucayr3obhafuyuo00aewdyvun/embed start=false& loop=true& delayms=3000" width="100%" height="360" frameborder="0" allowfullscreen="allowfullscreen"></iframe> icij developer miguel fiandor handles data harvesting on a much grander scale trawling hundreds of thousands of financial documents. fiandor\xe2\x80\x99s process begins by opening google devtools in the chrome browser. it\xe2\x80\x99s a mode that allows the user to see the inner workings of a website and play around with its code. then he uses the \xe2\x80\x98network\xe2\x80\x99 tab in the developer tools window to find the exact request he needs. (a request is how a browser retrieves a webpage\xe2\x80\x99s files from the website\xe2\x80\x99s servers.) he studies the communication between the website and his browser and isolates the requests he wants to target. fiandor tests those requests with curl a linux command that he can use from his computer terminal. this bypasses the need for a browser. next fiandor uses the <a href="https://medium.freecodecamp.org/how-to-scrape-websites-with-python-and-beautifulsoup-5946935d93fe">beautifulsoup library</a> that needs to be downloaded through python. beautifulsoup allows the user to parse html or separate it into useful elements. after the request he\xe2\x80\x99ll save the data onto his computer then route those elements into a spreadsheet and run his script. simple enough right </div></div>